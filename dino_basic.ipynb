{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18288"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform, zones=['Z1', 'Z2'], class_map_path='./data/classes.json'):\n",
    "        with open(class_map_path, 'r') as f:\n",
    "            self.class_map = {v:int(k) for k, v in json.load(f).items()}\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.classes = [d for d in os.listdir(root_dir)]\n",
    "        self.image_files = []\n",
    "        self.transform = transform\n",
    "        for c in self.classes:\n",
    "            for img in os.listdir(os.path.join(root_dir, c)):\n",
    "                if any(z for z in zones if z in img):\n",
    "                    self.image_files.append((os.path.join(root_dir, c, img), self.class_map[c]))\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, c = self.image_files[idx]\n",
    "        img_np = np.array(Image.open(img_path))[:,:,:3]\n",
    "        transformed = self.transform(image=img_np)['image']\n",
    "        return self.toTensor(transformed), torch.tensor(c)\n",
    "\n",
    "# These are the mean/std I took from the complete tiff of Z1\n",
    "ADE_MEAN = np.array([51.61087416176021, 70.54108897685563, 43.65073194868197]) / 255\n",
    "ADE_STD = np.array([66.21302035582556, 82.09431586857384, 54.93294965405881]) / 255\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "train_dataset = TreeDataset('./data/tree_classification_with_background', train_transform)\n",
    "val_dataset = TreeDataset('./data/tree_classification_with_background', val_transform, zones=['Z3'])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Pick this up tomorrow: https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DINOv2/Train_a_linear_classifier_on_top_of_DINOv2_for_semantic_segmentation.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs[0]\n",
    "\n",
    "# We have to force return_dict=False for tracing\n",
    "model.config.return_dict = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    traced_model = torch.jit.trace(model, [inputs.pixel_values])\n",
    "    traced_outputs = traced_model(inputs.pixel_values)\n",
    "\n",
    "print((last_hidden_states - traced_outputs[0]).abs().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
