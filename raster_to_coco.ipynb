{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio.windows import Window\n",
    "import uuid\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "from rasterio.features import geometry_mask\n",
    "from pycocotools import mask as coco_mask\n",
    "from datetime import datetime\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Proper labels\n",
    "The labels in the Laurentian data are strings, but COCO works better if these are abstracted by integer classes. So we need to replace the strings with integers, and then save a map somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to integer mapping saved to ./data/classes.json\n"
     ]
    }
   ],
   "source": [
    "def map_labels_to_integers(shapefile_paths, output_json_path):\n",
    "    '''\n",
    "    Runs an initial pass over all the shapefiles to get a list of classes, then\n",
    "    maps them to integers for later use. Saves the map to a JSON file.\n",
    "\n",
    "    Note: This also modifies the shapefiles by adding a 'Label' integer column.\n",
    "    Back up the shapefiles before doing this, if it matters.\n",
    "    '''\n",
    "    label_to_int_map = {}\n",
    "    int_to_label_map = {}\n",
    "    next_int = 0  # Start mapping from integer 0\n",
    "    \n",
    "    for path in shapefile_paths:\n",
    "        # Read the shapefile\n",
    "        gdf = gpd.read_file(path)\n",
    "\n",
    "        # Check if the 'Label' column exists\n",
    "        if 'Label' not in gdf.columns:\n",
    "            raise ValueError(f\"The shapefile {path} does not contain a 'Label' column.\")\n",
    "        \n",
    "        # Map the 'Label' to an integer if it's not already in the dictionary\n",
    "        for label in gdf['Label']:\n",
    "            if label not in label_to_int_map:\n",
    "                label_to_int_map[label] = next_int\n",
    "                int_to_label_map[next_int] = label\n",
    "                next_int += 1\n",
    "        \n",
    "        # Replace the 'Label' column with the mapped integers\n",
    "        gdf['class'] = gdf['Label'].map(label_to_int_map)\n",
    "\n",
    "        # Save the modified shapefile\n",
    "        gdf.to_file(path)\n",
    "\n",
    "    # Save the label to integer mapping to a JSON file\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(int_to_label_map, f, indent=4)\n",
    "\n",
    "    print(f\"Label to integer mapping saved to {output_json_path}\")\n",
    "\n",
    "\n",
    "shapefile_paths = ['./data/Z1_polygons.gpkg', './data/Z2_polygons.gpkg', './data/Z3_polygons.gpkg']\n",
    "output_json_path = './data/classes.json'\n",
    "map_labels_to_integers(shapefile_paths, output_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Processing to COCO\n",
    "There's a lot going on here, so I'll try to summarize it.\n",
    "\n",
    "First, I have clipped the rasters a bit. There was some area around the edges that did not have labels, so they have been cut out. I only want to have trees in this dataset if they're labelled. This is why the geotiff paths end with `rgb-clipped.tif`.\n",
    "\n",
    "We iterate through the (clipped geotiff, shapefile) pairs for each zone (Z1, Z2, Z3). For each geotiff, we need to process it in tiles. The tile width and height can be set at the top.\n",
    "\n",
    "Each tile is given a UUID. This is necessary in case we want to add more datasets down the line, and avoid name collisions. We use `gpd.overlay` to calculate which shapefiles fall within this tile. We then do a bit of filtering to remove tiny bits of trees that are mostly not in the tile (this is the `intersecting_polygons = intersecting_polygons[intersecting_polygons['area'] >= 2.0]`).\n",
    "\n",
    "The mask tool from `pycocotools` is used to do run-length encoding (RLE) on each polygon, after which we save it into an annotation entry. All info is appended to the coco JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_raster(geotiffs, shapefiles, name):\n",
    "    if not os.path.exists(f'data/chunked/{name}'):\n",
    "        os.mkdir(f'data/chunked/{name}')\n",
    "    tile_width, tile_height = 1024, 1024\n",
    "\n",
    "    # counters - we will need to modify these if we want to append more data to this dataset.\n",
    "    tile_id = 0\n",
    "    annotation_id = 0\n",
    "\n",
    "    class_map_file = 'data/classes.json'\n",
    "\n",
    "    with open(class_map_file, 'r') as f:\n",
    "        class_map = json.load(f)\n",
    "\n",
    "    coco_dict = {\n",
    "        'info': [],\n",
    "        'licenses': [],\n",
    "        'images': [],\n",
    "        'annotations': [],\n",
    "        'categories': [{ 'id': idx, 'name': namex, 'supercategory': 'tree'} for idx, namex in class_map.items()],\n",
    "    }\n",
    "\n",
    "    for file, shapes in zip(geotiffs, shapefiles):\n",
    "        \n",
    "        gdf = gpd.read_file(shapes)\n",
    "\n",
    "        # Create tiles from raster\n",
    "        with rio.open(file) as tif:\n",
    "            n_tiles_x = tif.width // tile_width\n",
    "            n_tiles_y = tif.height // tile_height\n",
    "\n",
    "            print(f'Parsing {n_tiles_y} x {n_tiles_x} tiles for {file}...')\n",
    "\n",
    "            for j in range (n_tiles_y):\n",
    "                print(f'  row {j}...')\n",
    "                for i in range(n_tiles_x):\n",
    "                    win_x = i * tile_width\n",
    "                    win_y = j * tile_height\n",
    "                    window = Window(win_x, win_y, tile_width, tile_height)\n",
    "                    raster = tif.read(window=window)\n",
    "\n",
    "                    # If it's >50% black pixels, just continue. No point segmenting it.\n",
    "                    if np.sum(raster == 0) / (tile_width * tile_height * tif.count) > 0.5:\n",
    "                        continue\n",
    "\n",
    "                    window_transform = tif.window_transform(window)\n",
    "                    tile_name = f'{uuid.uuid4()}.tif'\n",
    "\n",
    "                    with rio.open(\n",
    "                        f'data/chunked/{name}/{tile_name}', 'w',\n",
    "                        driver='GTiff',\n",
    "                        height=tile_height,\n",
    "                        width=tile_width,\n",
    "                        count=tif.count,\n",
    "                        dtype=tif.dtypes[0],\n",
    "                        crs=tif.crs,\n",
    "                        transform=window_transform,\n",
    "                    ) as tile_raster:\n",
    "                        tile_raster.write(raster)\n",
    "                        coco_dict['images'].append({\n",
    "                            'id': tile_id,\n",
    "                            'width': tile_width,\n",
    "                            'height': tile_height,\n",
    "                            'gsd': 0.0192, # resolution of the laurentian data\n",
    "                            'file_name': tile_name,\n",
    "                            'date_captured': '2022-06-01',\n",
    "                        })\n",
    "\n",
    "                        tile_annotations = []\n",
    "\n",
    "                        # Get bbox for the whole tile, then use that to find relevant shape masks\n",
    "                        bbox_polygon = gpd.GeoDataFrame(gpd.GeoSeries(box(*tile_raster.bounds)), columns=['geometry'], crs=tif.crs)\n",
    "                        intersecting_polygons = gpd.overlay(gdf, bbox_polygon, how='intersection')\n",
    "                        intersecting_polygons['area'] = intersecting_polygons.geometry.area\n",
    "\n",
    "                        # We should drop tiny bits of masks sticking into the frame\n",
    "                        intersecting_polygons = intersecting_polygons[intersecting_polygons['area'] >= 2.0]\n",
    "                        intersecting_polygons.reset_index(inplace=True, drop=True)\n",
    "\n",
    "                        for i, shp in intersecting_polygons.iterrows():\n",
    "                            mask = geometry_mask([shp.geometry], transform=window_transform, invert=True, out_shape=(tile_height, tile_width)).astype(np.uint8)\n",
    "                            segmentation = coco_mask.encode(np.asfortranarray(mask))\n",
    "                            segmentation['counts'] = segmentation['counts'].decode('ascii')\n",
    "                            #segmentation['counts'] = base64.b64encode(segmentation['counts']).decode('utf-8')\n",
    "\n",
    "                            # Get bounding box - some dubious operations here\n",
    "                            minx_lat, miny_lat, maxx_lat, maxy_lat = shp.geometry.bounds\n",
    "                            miny, minx = rio.transform.rowcol(window_transform, minx_lat, miny_lat)\n",
    "                            maxy, maxx = rio.transform.rowcol(window_transform, maxx_lat, maxy_lat)\n",
    "\n",
    "                            # Annotation entry for this shape\n",
    "                            tile_annotations.append({\n",
    "                                'id': annotation_id,\n",
    "                                'image_id': tile_id,\n",
    "                                'category_id': shp['class'],\n",
    "                                'segmentation': segmentation,\n",
    "                                'area': shp['area'],\n",
    "                                'bbox': [minx, miny, abs(maxx-minx), abs(maxy-miny)],\n",
    "                                'iscrowd': 0,\n",
    "                            })\n",
    "                            annotation_id += 1\n",
    "                        \n",
    "                        tile_id += 1\n",
    "                        coco_dict['annotations'].extend(tile_annotations)\n",
    "\n",
    "                        '''\n",
    "                        # Some graphing code in case I need it again\n",
    "                        fig, ax = plt.subplots(figsize=(10,10))\n",
    "                        show(raster, ax=ax, transform=window_transform)\n",
    "                        show(mask, ax=ax, transform=window_transform, alpha=0.5)\n",
    "                        #intersecting_polygons.loc[[8]].plot(ax=ax, edgecolor='black', alpha=0.5)\n",
    "                        '''\n",
    "            \n",
    "            # Write JSON after every file, for funsies\n",
    "            with open(f'data/chunked/{name}.json', 'w') as f:\n",
    "                json.dump(coco_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 30 x 29 tiles for ./data/2021-06-17/zone1/2021-06-17-sbl-z1-rgb-clipped.tif...\n",
      "  row 0...\n",
      "  row 1...\n",
      "  row 2...\n",
      "  row 3...\n",
      "  row 4...\n",
      "  row 5...\n",
      "  row 6...\n",
      "  row 7...\n",
      "  row 8...\n",
      "  row 9...\n",
      "  row 10...\n",
      "  row 11...\n",
      "  row 12...\n",
      "  row 13...\n",
      "  row 14...\n",
      "  row 15...\n",
      "  row 16...\n",
      "  row 17...\n",
      "  row 18...\n",
      "  row 19...\n",
      "  row 20...\n",
      "  row 21...\n",
      "  row 22...\n",
      "  row 23...\n",
      "  row 24...\n",
      "  row 25...\n",
      "  row 26...\n",
      "  row 27...\n",
      "  row 28...\n",
      "  row 29...\n",
      "Parsing 19 x 29 tiles for ./data/2021-06-17/zone2/2021-06-17-sbl-z2-rgb-clipped.tif...\n",
      "  row 0...\n",
      "  row 1...\n",
      "  row 2...\n",
      "  row 3...\n",
      "  row 4...\n",
      "  row 5...\n",
      "  row 6...\n",
      "  row 7...\n",
      "  row 8...\n",
      "  row 9...\n",
      "  row 10...\n",
      "  row 11...\n",
      "  row 12...\n",
      "  row 13...\n",
      "  row 14...\n",
      "  row 15...\n",
      "  row 16...\n",
      "  row 17...\n",
      "  row 18...\n",
      "Parsing 18 x 17 tiles for ./data/2021-06-17/zone3/2021-06-17-sbl-z3-rgb-clipped.tif...\n",
      "  row 0...\n",
      "  row 1...\n",
      "  row 2...\n",
      "  row 3...\n",
      "  row 4...\n",
      "  row 5...\n",
      "  row 6...\n",
      "  row 7...\n",
      "  row 8...\n",
      "  row 9...\n",
      "  row 10...\n",
      "  row 11...\n",
      "  row 12...\n",
      "  row 13...\n",
      "  row 14...\n",
      "  row 15...\n",
      "  row 16...\n",
      "  row 17...\n"
     ]
    }
   ],
   "source": [
    "train_geotiff_paths = ['./data/2021-06-17/zone1/2021-06-17-sbl-z1-rgb-clipped.tif', './data/2021-06-17/zone2/2021-06-17-sbl-z2-rgb-clipped.tif']\n",
    "train_shapefile_paths = ['./data/Z1_polygons.gpkg', './data/Z2_polygons.gpkg']\n",
    "\n",
    "val_geotiff_paths = ['./data/2021-06-17/zone3/2021-06-17-sbl-z3-rgb-clipped.tif']\n",
    "val_shapefile_paths = ['./data/Z3_polygons.gpkg']\n",
    "\n",
    "chunk_raster(train_geotiff_paths, train_shapefile_paths, 'train')\n",
    "chunk_raster(val_geotiff_paths, val_shapefile_paths, 'val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
